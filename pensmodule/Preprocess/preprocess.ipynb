{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "import json\n",
    "import time\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfilename = '../../data/train.tsv'\n",
    "validfilename = '../../data/valid.tsv'\n",
    "testfilename = '../../data/personalized_test.tsv'\n",
    "docsfilename = '../../data/news.tsv'\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_FREQ_THRESHOLD = 3\n",
    "MAX_CONTENT_LEN = 500\n",
    "MAX_BODY_LEN = 100\n",
    "MAX_TITLE_LEN = 16\n",
    "WORD_EMBEDDING_DIM = 300\n",
    "MAX_CLICK_LEN = 50\n",
    "\n",
    "word2freq = {}\n",
    "word2index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(sent):\n",
    "    pat = re.compile(r'[\\w]+|[.,!?;|]')\n",
    "    if isinstance(sent, str):\n",
    "        return pat.findall(sent.lower())\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_news(filename,filer_num=3):\n",
    "    \"\"\"\n",
    "    Input: Takes news file\n",
    "    Output: \n",
    "        news: newsid_newsdetail map: {'N10000': [\"category\", [\"tokenized\", \"title\", \"...\"], [\"tokenized\", \"body\", \"...\"] ]\n",
    "        news_index: newsid_index map {'N10000': 1, 'N10001': 2,...}\n",
    "        category_dict: category_index map {\"cat1\": 1, \"cat2\":2,...}\n",
    "        word_dict: word_index map {\"word1\": 3, \"word2\":4...}\n",
    "    \"\"\"\n",
    "    news={}\n",
    "    category, subcategory=[], []\n",
    "    news_index={}\n",
    "    index=1\n",
    "    word_cnt=Counter()\n",
    "    err = 0\n",
    "    news_data = pd.read_csv(filename, sep='\\t')\n",
    "    news_data.fillna(value=\" \", inplace=True)\n",
    "    for i in tqdm(range(len(news_data))):\n",
    "        doc_id,vert,_, title, snipplet= news_data.loc[i,:][:5]\n",
    "        news_index[doc_id]=index\n",
    "        index+=1\n",
    "\n",
    "        title = title.lower()\n",
    "        title = word_tokenize(title)\n",
    "        snipplet = snipplet.lower()\n",
    "        snipplet = word_tokenize(snipplet)\n",
    "        category.append(vert)\n",
    "        news[doc_id] = [vert,title,snipplet]     \n",
    "        word_cnt.update(snipplet+title)\n",
    "    # 0: pad; 1: <sos>; 2: <eos>\n",
    "    word = [k for k , v in word_cnt.items() if v >= filer_num]\n",
    "    word_dict = {k:v for k, v in zip(word, range(3,len(word)+3))}\n",
    "    category=list(set(category))\n",
    "    category_dict={k:v for k, v in zip(category, range(1,len(category)+1))}\n",
    "\n",
    "    return news,news_index,category_dict,word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 113762/113762 [00:24<00:00, 4732.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.7 s, sys: 5.75 s, total: 48.5 s\n",
      "Wall time: 48.7 s\n"
     ]
    }
   ],
   "source": [
    "%time news,news_index,category_dict,word_dict = read_news(docsfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sports'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# min(news_index.keys()), max(news_index.keys())\n",
    "list(news.values())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict['unk'] = 0\n",
    "word_dict['<sos>'] = 1\n",
    "word_dict['<eos>'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: avoid pickle, use parquet or csv instead \n",
    "with open('../../data2/dict.pkl', 'wb') as f:\n",
    "    pickle.dump([news_index,category_dict,word_dict], f)\n",
    "with open('../../data2/news.pkl', 'wb') as f:\n",
    "    pickle.dump(news, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get inputs for user encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_rep_for_userencoder(news,news_index,category_dict,word_dict):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        news: newsid_newsdetail map: {'N10000': [\"category\", [\"tokenized\", \"title\", \"...\"], [\"tokenized\", \"body\", \"...\"] ]\n",
    "        news_index: newsid_index map {'N10000': 1, 'N10001': 2,...}\n",
    "        category_dict: category_index map {\"cat1\": 1, \"cat2\":2,...}\n",
    "        word_dict: word_index map {\"word1\": 3, \"word2\":4...}\n",
    "    Output:\n",
    "        news_vert: Numerical representation of category of each article. numpy tensor of shape (news_num).\n",
    "        every article has single catgory hence represented by single number \n",
    "        news_title: Numerical representation of all titles. numpy tensor of shape (news_num, MAX_TITLE_LEN)\n",
    "        news_body: Numerical representation of all news bodies. numpy tensor of shape (news_num, MAX_BODY_LEN)\n",
    "    \"\"\"\n",
    "    news_num=len(news)+1\n",
    "    news_title=np.zeros((news_num,MAX_TITLE_LEN),dtype='int32')\n",
    "    news_body=np.zeros((news_num,MAX_BODY_LEN),dtype='int32')\n",
    "    news_vert=np.zeros((news_num),dtype='int32')\n",
    "    for key in news:    \n",
    "        vert,title,body=news[key]\n",
    "        doc_index=news_index[key]\n",
    "        # encode category\n",
    "        news_vert[doc_index] = category_dict[vert]\n",
    "        \n",
    "        # encode title\n",
    "        counter = 0\n",
    "        for word_id in range(min(MAX_TITLE_LEN,len(title))):\n",
    "            if title[word_id] in word_dict:\n",
    "                news_title[doc_index,counter]=word_dict[title[word_id].lower()]\n",
    "                counter += 1\n",
    "        \n",
    "        # encode body\n",
    "        counter = 0\n",
    "        for word_id in range(min(MAX_BODY_LEN,len(body))):\n",
    "            if body[word_id] in word_dict:\n",
    "                news_body[doc_index,counter]=word_dict[body[word_id].lower()]\n",
    "                counter += 1\n",
    "    return news_vert, news_title, news_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.22 s, sys: 0 ns, total: 3.22 s\n",
      "Wall time: 3.22 s\n"
     ]
    }
   ],
   "source": [
    "%time news_vert, news_title, news_body = get_rep_for_userencoder(news,news_index,category_dict,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([298, 299, 407, 303, 301, 122, 324, 325, 326,   0,   0,   0,   0,\n",
       "         0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(news_vert),len(news_title), len(news_body)\n",
    "news_title[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data2/news_vert.npy', news_vert)\n",
    "np.save('../../data2/news_title.npy', news_title)\n",
    "np.save('../../data2/news_body.npy', news_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get inputs/ targets for seq2seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rep_for_seq2seq(news,news_index,word_dict):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        news: newsid_newsdetail map: {'N10000': [\"category\", [\"tokenized\", \"title\", \"...\"], [\"tokenized\", \"body\", \"...\"] ]\n",
    "        news_index: newsid_index map {'N10000': 1, 'N10001': 2,...}\n",
    "        word_dict: word_index map {\"word1\": 3, \"word2\":4...}\n",
    "    output:\n",
    "        sources: numpy tensor of shape (news_num,MAX_CONTENT_LEN)\n",
    "        target_inputs: numpy tensor of shape (news_num,MAX_TITLE_LEN)\n",
    "        target_outputs: numpy tensor of shape (news_num,MAX_TITLE_LEN) same as target inputs left shifted by one\n",
    "    \"\"\"\n",
    "    news_num=len(news)+1\n",
    "    sources=np.zeros((news_num,MAX_CONTENT_LEN),dtype='int32')\n",
    "    target_inputs=np.zeros((news_num,MAX_TITLE_LEN),dtype='int32')\n",
    "    target_outputs=np.zeros((news_num,MAX_TITLE_LEN),dtype='int32')\n",
    "    \n",
    "    for key in tqdm(news):    \n",
    "        _, title, body = news[key]\n",
    "        doc_index = news_index[key]\n",
    "        \n",
    "        # get numerical indexes of body words from word_dict   \n",
    "        counter = 0\n",
    "        for word_id in range(min(MAX_CONTENT_LEN-1,len(body))):\n",
    "            if body[word_id] in word_dict:\n",
    "                sources[doc_index,counter]=word_dict[body[word_id].lower()]\n",
    "                counter += 1\n",
    "        sources[doc_index,counter] = 2 \n",
    "        \n",
    "        # get numerical indexes of title words from word_dict\n",
    "        target_inputs[doc_index,0] = 1 # set first word as <sos>\n",
    "        counter = 1\n",
    "        for word_id in range(min(MAX_TITLE_LEN-1,len(title))):\n",
    "            if title[word_id] in word_dict:\n",
    "                target_inputs[doc_index,counter]=word_dict[title[word_id].lower()]\n",
    "                counter += 1\n",
    "\n",
    "        # get numerical indexes of title words from word_dict   \n",
    "        counter = 0\n",
    "        for word_id in range(min(MAX_TITLE_LEN-1,len(title))):\n",
    "            if title[word_id] in word_dict:\n",
    "                target_outputs[doc_index,counter]=word_dict[title[word_id].lower()]\n",
    "                counter += 1\n",
    "        target_outputs[doc_index,counter] = 2 # set last target word as <eos>\n",
    "        \n",
    "    return sources, target_inputs, target_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 113762/113762 [00:11<00:00, 10123.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 172 ms, total: 11.3 s\n",
      "Wall time: 11.2 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time sources, target_inputs, target_outputs = get_rep_for_seq2seq(news,news_index,word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1 298 299 407 303 301 122 324 325 326   0   0   0   0   0   0]\n",
      "[298 299 407 303 301 122 324 325 326   2   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# sources[2]\n",
    "print(target_inputs[2])\n",
    "print(target_outputs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data2/sources.npy', sources)\n",
    "np.save('../../data2/target_inputs.npy', target_inputs)\n",
    "np.save('../../data2/target_outputs.npy', target_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matrix(embedding_path,word_dict):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        embedding_path: path where model is stored \n",
    "        word_dict: word_index map\n",
    "    output:\n",
    "        embedding_matrix: tensor of shape (word_dict, 300) embedding corresponding to each vocab word available. \n",
    "        if unavailable, random values between 0 and 0.1\n",
    "        have_word: list of words in vocab which are available in embedding model.\n",
    "    \"\"\"\n",
    "    mu, sigma = 0, 0.1\n",
    "    embedding_zero = np.zeros((1,300))\n",
    "    embedding_matrix = np.random.normal(mu, sigma, (len(word_dict)-1, WORD_EMBEDDING_DIM))\n",
    "    embedding_matrix = np.concatenate((embedding_zero,embedding_matrix))\n",
    "    have_word=[]\n",
    "    with open(os.path.join(embedding_path,'glove.840B.300d.txt'),'rb') as f:\n",
    "        while True:\n",
    "            l=f.readline()\n",
    "            if len(l)==0:\n",
    "                break\n",
    "            l=l.split()\n",
    "            word = l[0].decode()\n",
    "            if word in word_dict:\n",
    "                index = word_dict[word]\n",
    "                tp = [float(x) for x in l[1:]]\n",
    "                embedding_matrix[index]=np.array(tp)\n",
    "                have_word.append(word)\n",
    "    return embedding_matrix,have_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.9 s, sys: 4.32 s, total: 31.2 s\n",
      "Wall time: 35.4 s\n"
     ]
    }
   ],
   "source": [
    "%time embedding_matrix, have_word = load_matrix('../../data',word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141910, 300)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape\n",
    "# len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141910, 100875)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict),len(have_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../data2/embedding_matrix.npy', embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get train/ valid/ test examples from user logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Doc2ID(doclist,news2id):\n",
    "    \"\"\"\n",
    "    convert news_id list  to index_id list using lookup dictionary news2id \n",
    "    \"\"\"\n",
    "    return [news2id[i] for i in doclist if i in news2id ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PadDoc(doclist):\n",
    "    \"\"\"\n",
    "    pad document list from left side according to MAX_CLICK_LEN \n",
    "    \"\"\"\n",
    "    if len(doclist) >= MAX_CLICK_LEN:\n",
    "        return doclist[-MAX_CLICK_LEN:]\n",
    "    else:\n",
    "        return [0] * (MAX_CLICK_LEN-len(doclist)) + doclist[:MAX_CLICK_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user2dict(users):\n",
    "    user_set = set(users)\n",
    "    user_dict = {k:v for k, v in zip(user_set, range(0,len(user_set)))}\n",
    "    return user_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_train_user(filename,news_index):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        filename: path to train/test/valid filename\n",
    "        news_index: newsid_index map {'N10000': 1, 'N10001': 2,...}\n",
    "    output:\n",
    "        train_users: list of list. [[0padded, click, news, indexes],...]\n",
    "        train_samples: [userId, [news_id1, news_id2], [1(pos), 0(neg)] \n",
    "        \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    df.fillna(value=\" \", inplace=True)\n",
    "\n",
    "    # convert news id to index id and perform padding\n",
    "    df['ClicknewsID'] = df['ClicknewsID'].apply(lambda x: PadDoc(Doc2ID(x.split(),news_index)))\n",
    "\n",
    "    # convert pos, neg news_id to index_id\n",
    "    df['pos']  = df['pos'].apply(lambda x: Doc2ID(x.split(),news_index))\n",
    "    df['neg'] = df['neg'].apply(lambda x: Doc2ID(x.split(),news_index))\n",
    "    \n",
    "    pos_neg_lists = []\n",
    "    \n",
    "    for userindex, (pos_list, neg_list) in tqdm(enumerate(zip(df['pos'].values.tolist(), df['neg'].values.tolist()))):\n",
    "        \"\"\"\n",
    "        For each user\n",
    "        1. Shuffle pos and neg list\n",
    "        2. Ensure positive negative lists are of same length\n",
    "        \"\"\"\n",
    "        if len(pos_list) and len(neg_list):\n",
    "            # sampling 1 negative sample for 1 pos sample\n",
    "            min_len = min(len(pos_list), len(neg_list))\n",
    "            np.random.shuffle(pos_list)\n",
    "            np.random.shuffle(neg_list)\n",
    "            for i in range(min_len):\n",
    "                pos_neg_lists.append([userindex, [pos_list[i],neg_list[i]],[1,0]])\n",
    "        \n",
    "    return df['ClicknewsID'].values.tolist(), pos_neg_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:04, 97143.54it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.9 s, sys: 1.6 s, total: 23.5 s\n",
      "Wall time: 24.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%time TrainUsers, TrainSamples = parse_train_user(trainfilename, news_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "763188\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0, [88304, 90055], [1, 0]]]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(TrainSamples[0])\n",
    "print(len(TrainUsers))\n",
    "print(len(TrainSamples))\n",
    "[s for s in TrainSamples[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data2/TrainUsers.pkl', 'wb') as f:\n",
    "    pickle.dump(TrainUsers, f)\n",
    "with open('../../data2/TrainSamples.pkl', 'wb') as f:\n",
    "    pickle.dump(TrainSamples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_valid_user(filename,news_index):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        filename: path to validation file\n",
    "        news_index: newsid_index map {'N10000': 1, 'N10001': 2,...}\n",
    "    output:\n",
    "        ValidUsers: list of list. [[0padded, click, news, indexes],...]\n",
    "        ValidSamples: list of list. [userId, [news_id1, news_id2, ..., news_idn], [pos_neglabel1, pos_neglabel2, ..., pos_neglabeln] \n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    df.fillna(value=\" \", inplace=True)\n",
    "    \n",
    "    df['ClicknewsID'] = df['ClicknewsID'].apply(lambda x: PadDoc(Doc2ID(x.split(),news_index)))\n",
    "    \n",
    "    df['pos']  = df['pos'].apply(lambda x: Doc2ID(x.split(),news_index))\n",
    "    df['neg'] = df['neg'].apply(lambda x: Doc2ID(x.split(),news_index))\n",
    "    \n",
    "    pos_neg_lists = []\n",
    "    for userindex, (pos_list, neg_list) in enumerate(zip(df['pos'].values.tolist(), df['neg'].values.tolist())):\n",
    "        if len(pos_list) and len(neg_list):\n",
    "            pos_neg_lists.append([userindex, pos_list+neg_list,[1]*len(pos_list)+[0]*len(neg_list)])\n",
    "        \n",
    "    return df['ClicknewsID'].values.tolist(), pos_neg_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.81 s, sys: 391 ms, total: 5.2 s\n",
      "Wall time: 5.27 s\n"
     ]
    }
   ],
   "source": [
    "%time ValidUsers, ValidSamples = parse_valid_user(validfilename,news_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data2/ValidUsers.pkl', 'wb') as f:\n",
    "    pickle.dump(ValidUsers, f)\n",
    "with open('../../data2/ValidSamples.pkl', 'wb') as f:\n",
    "    pickle.dump(ValidSamples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test_user(filename,news_index):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        filename: path to test file\n",
    "        news_index: newsid_index map {'N10000': 1, 'N10001': 2,...}\n",
    "    output:\n",
    "        TestUsers:list of list. [[0padded, click, news, indexes],...]\n",
    "        TestSamples: list of list. [[userindex, pos_id, rewrite_title]]\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, sep='\\t')\n",
    "    \n",
    "    df['clicknewsID'] = df['clicknewsID'].apply(lambda x: PadDoc(Doc2ID(x.split(','),news_index)))\n",
    "    \n",
    "    df['posnewID']  = df['posnewID'].apply(lambda x: Doc2ID(x.split(','),news_index))\n",
    "    \n",
    "    df['rewrite_titles'] = df['rewrite_titles'].apply(lambda x: [i.lower() for i in x.split(';;')] )\n",
    "    \n",
    "    pos_lists = []\n",
    "    for userindex, (pos_lis, rewrite_title_lis) in enumerate(zip(df['posnewID'].values.tolist(), df['rewrite_titles'].values.tolist())):\n",
    "        for pos, rewrite_title in zip(pos_lis, rewrite_title_lis):\n",
    "            if rewrite_title.strip() == '':\n",
    "                continue\n",
    "            else:\n",
    "                pos_lists.append([userindex, pos, rewrite_title])\n",
    "    \n",
    "    return df['clicknewsID'].values.tolist(), pos_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.1 ms, sys: 3.02 ms, total: 33.2 ms\n",
      "Wall time: 32.5 ms\n"
     ]
    }
   ],
   "source": [
    "%time TestUsers, TestSamples = parse_test_user(testfilename,news_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 14111,\n",
       " \"legal battle looms over trump epa's rule change of obama's clean power plan rule\"]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestSamples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../data2/TestUsers.pkl', 'wb') as f:\n",
    "    pickle.dump(TestUsers, f)\n",
    "with open('../../data2/TestSamples.pkl', 'wb') as f:\n",
    "    pickle.dump(TestSamples, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestSamples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1ea14d0bd9c7a0f4f2d255c0662c7e1119328c505d1c17d9d8f159415dfcf69"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
